#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å› å­é¢„å¤„ç†æ¨¡å—
å‚è€ƒalphasickleé¡¹ç›®çš„å› å­é¢„å¤„ç†æ–¹æ³•ï¼Œå¯¹dws_stock_factorsè¡¨è¿›è¡Œé¢„å¤„ç†
åŒ…æ‹¬ï¼šç¼ºå¤±å€¼å¤„ç†ã€å»æå€¼å¤„ç†ã€æ ‡å‡†åŒ–å¤„ç†ã€ä¸­æ€§åŒ–å¤„ç†
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from database.manager_fixed import DatabaseManagerFixed
from sqlalchemy import text
import logging
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)


class FactorPreprocessor:
    """å› å­é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self.db_manager = DatabaseManagerFixed()
        
        # åŠ¨æ€è¯†åˆ«çš„å› å­åˆ—ï¼ˆè¿è¡Œæ—¶ç¡®å®šï¼‰
        self.factor_columns = []
        
        # éœ€è¦ä¸­æ€§åŒ–çš„å› å­ï¼ˆåŠ¨æ€è¯†åˆ«ï¼‰
        self.neutralize_factor_list = []
        
        # åŸºç¡€ä¿¡æ¯åˆ—ï¼ˆéå› å­åˆ—ï¼‰
        self.base_columns = ['code', 'date', 'industry', 'code_name', 'close', 'volume', 'amount', 'pctChg']
        
        # æ’é™¤çš„åˆ—ï¼ˆä¸å‚ä¸å› å­é¢„å¤„ç†ï¼‰
        self.exclude_columns = [
            'code', 'date', 'industry', 'code_name', 'close', 'volume', 'amount', 'pctChg',
            'created_at', 'updated_at', 'pubDate', 'statDate', 'frequency', 'open', 'high', 'low', 
            'preclose', 'adjustflag', 'turn', 'tradestatus', 'isST', 'totalShare', 'liqaShare',
            # åŸå§‹æ•°æ®åˆ—ï¼ˆä¸å‚ä¸å› å­é¢„å¤„ç†ï¼‰
            'peTTM', 'pbMRQ', 'psTTM', 'pcfNcfTTM', 'roeAvg', 'npMargin', 'gpMargin', 'netProfit', 
            'epsTTM', 'MBRevenue', 'currentRatio', 'quickRatio', 'cashRatio', 'liabilityToAsset', 
            'assetToEquity', 'CAToAsset', 'NCAToAsset', 'ebitToInterest', 'CFOToOR', 'CFOToNP', 
            'NRTurnRatio', 'INVTurnRatio', 'CATurnRatio', 'AssetTurnRatio', 'YOYEquity', 'YOYAsset', 
            'YOYNI', 'YOYEPSBasic', 'dupontROE', 'dupontAssetStoEquity', 'dupontAssetTurn'
        ]
    
    def detect_factor_columns(self, table_name: str = 'dws_stock_factors') -> List[str]:
        """
        åŠ¨æ€æ£€æµ‹å› å­è¡¨ä¸­çš„å› å­åˆ—
        
        Args:
            table_name: å› å­è¡¨å
            
        Returns:
            å› å­åˆ—ååˆ—è¡¨
        """
        with self.db_manager.engine.connect() as conn:
            # è·å–è¡¨ç»“æ„
            query = f"DESCRIBE {table_name}"
            result = conn.execute(text(query))
            columns_info = result.fetchall()
            
            # æå–åˆ—å
            all_columns = [row[0] for row in columns_info]
            
            # è¯†åˆ«å› å­åˆ—ï¼ˆæ’é™¤åŸºç¡€ä¿¡æ¯åˆ—ï¼‰
            factor_columns = []
            for col in all_columns:
                if col not in self.exclude_columns:
                    factor_columns.append(col)
            
            logger.info(f"æ£€æµ‹åˆ° {len(factor_columns)} ä¸ªå› å­åˆ—: {factor_columns[:10]}{'...' if len(factor_columns) > 10 else ''}")
            return factor_columns
    
    def get_factor_data(self, start_date: str, end_date: str, table_name: str = 'dws_stock_factors') -> pd.DataFrame:
        """
        è·å–å› å­æ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            table_name: å› å­è¡¨å
            
        Returns:
            å› å­æ•°æ®DataFrame
        """
        # æ¯æ¬¡é‡æ–°æ£€æµ‹å› å­åˆ—ï¼Œç¡®ä¿ä»æ­£ç¡®çš„è¡¨è·å–åˆ—ä¿¡æ¯
        factor_columns = self.detect_factor_columns(table_name)
        
        with self.db_manager.engine.connect() as conn:
            # æ„å»ºæŸ¥è¯¢SQLï¼ŒåŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
            factor_cols = ', '.join(factor_columns)
            query = f"""
            SELECT 
                code, date, industry, code_name,
                close, volume, amount, pctChg,
                {factor_cols}
            FROM {table_name}
            WHERE date BETWEEN %(start_date)s AND %(end_date)s
            ORDER BY code, date
            """
            
            df = pd.read_sql(query, conn, params={
                'start_date': start_date,
                'end_date': end_date
            })
            
            logger.info(f"è·å–å› å­æ•°æ®: {len(df)} æ¡è®°å½•ï¼Œ{len(factor_columns)} ä¸ªå› å­")
            return df
    
    def handle_missing_values(self, df: pd.DataFrame, method: str = 'forward_fill') -> pd.DataFrame:
        """
        å¤„ç†ç¼ºå¤±å€¼
        
        Args:
            df: å› å­æ•°æ®
            method: å¤„ç†æ–¹æ³• ('forward_fill', 'backward_fill', 'mean', 'median', 'drop')
            
        Returns:
            å¤„ç†åçš„æ•°æ®
        """
        logger.info(f"å¼€å§‹å¤„ç†ç¼ºå¤±å€¼ï¼Œæ–¹æ³•: {method}")
        
        df_processed = df.copy()
        
        # åŠ¨æ€è¯†åˆ«å› å­åˆ—ï¼ˆæ’é™¤åŸºç¡€ä¿¡æ¯åˆ—ï¼‰
        factor_columns = [col for col in df.columns if col not in self.exclude_columns]
        
        if method == 'forward_fill':
            # å‰å‘å¡«å……ï¼Œç„¶ååå‘å¡«å……å¤„ç†å‰©ä½™NULLå€¼
            df_processed[factor_columns] = df_processed.groupby('code')[factor_columns].fillna(method='ffill')
            df_processed[factor_columns] = df_processed.groupby('code')[factor_columns].fillna(method='bfill')
        elif method == 'backward_fill':
            # åå‘å¡«å……ï¼Œç„¶åå‰å‘å¡«å……å¤„ç†å‰©ä½™NULLå€¼
            df_processed[factor_columns] = df_processed.groupby('code')[factor_columns].fillna(method='bfill')
            df_processed[factor_columns] = df_processed.groupby('code')[factor_columns].fillna(method='ffill')
        elif method == 'mean':
            # å‡å€¼å¡«å……
            df_processed[factor_columns] = df_processed[factor_columns].fillna(df_processed[factor_columns].mean())
        elif method == 'median':
            # ä¸­ä½æ•°å¡«å……
            df_processed[factor_columns] = df_processed[factor_columns].fillna(df_processed[factor_columns].median())
        elif method == 'drop':
            # åˆ é™¤ç¼ºå¤±å€¼
            df_processed = df_processed.dropna(subset=factor_columns)
        
        # å¦‚æœè¿˜æœ‰NULLå€¼ï¼Œç”¨0å¡«å……
        remaining_nulls = df_processed[factor_columns].isnull().sum().sum()
        if remaining_nulls > 0:
            logger.warning(f"ä»æœ‰ {remaining_nulls} ä¸ªNULLå€¼ï¼Œç”¨0å¡«å……")
            df_processed[factor_columns] = df_processed[factor_columns].fillna(0)
        
        # ç»Ÿè®¡ç¼ºå¤±å€¼å¤„ç†æƒ…å†µ
        missing_before = df[factor_columns].isnull().sum().sum()
        missing_after = df_processed[factor_columns].isnull().sum().sum()
        logger.info(f"ç¼ºå¤±å€¼å¤„ç†å®Œæˆ: {missing_before} -> {missing_after}")
        
        return df_processed
    
    def winsorize_factors(self, df: pd.DataFrame, method: str = 'quantile', 
                         limits: Tuple[float, float] = (0.01, 0.99)) -> pd.DataFrame:
        """
        å»æå€¼å¤„ç†ï¼ˆWinsorizationï¼‰
        
        Args:
            df: å› å­æ•°æ®
            method: å»æå€¼æ–¹æ³• ('quantile', 'std', 'mad')
            limits: é™åˆ¶èŒƒå›´ï¼Œå¯¹äºquantileæ–¹æ³•ä¸º(ä¸‹åˆ†ä½æ•°, ä¸Šåˆ†ä½æ•°)ï¼Œå¯¹äºstd/madä¸º(ä¸‹é™å€æ•°, ä¸Šé™å€æ•°)
            
        Returns:
            å¤„ç†åçš„æ•°æ®
        """
        logger.info(f"å¼€å§‹å»æå€¼å¤„ç†ï¼Œæ–¹æ³•: {method}, é™åˆ¶: {limits}")
        
        df_processed = df.copy()
        
        # åŠ¨æ€è¯†åˆ«å› å­åˆ—ï¼ˆæ’é™¤åŸºç¡€ä¿¡æ¯åˆ—ï¼‰
        factor_columns = [col for col in df.columns if col not in self.exclude_columns]
        
        for col in factor_columns:
            if col not in df_processed.columns:
                continue
                
            series = df_processed[col].dropna()
            if len(series) == 0:
                continue
            
            if method == 'quantile':
                # åˆ†ä½æ•°æ–¹æ³•
                lower_limit = series.quantile(limits[0])
                upper_limit = series.quantile(limits[1])
            elif method == 'std':
                # æ ‡å‡†å·®æ–¹æ³•
                mean = series.mean()
                std = series.std()
                lower_limit = mean - limits[0] * std
                upper_limit = mean + limits[1] * std
            elif method == 'mad':
                # ä¸­ä½æ•°ç»å¯¹åå·®æ–¹æ³•
                median = series.median()
                mad = np.median(np.abs(series - median))
                lower_limit = median - limits[0] * mad
                upper_limit = median + limits[1] * mad
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å»æå€¼æ–¹æ³•: {method}")
            
            # åº”ç”¨é™åˆ¶
            df_processed[col] = np.clip(df_processed[col], lower_limit, upper_limit)
        
        logger.info("å»æå€¼å¤„ç†å®Œæˆ")
        return df_processed
    
    def standardize_factors(self, df: pd.DataFrame, method: str = 'zscore') -> pd.DataFrame:
        """
        æ ‡å‡†åŒ–å¤„ç†
        
        Args:
            df: å› å­æ•°æ®
            method: æ ‡å‡†åŒ–æ–¹æ³• ('zscore', 'minmax', 'robust')
            
        Returns:
            å¤„ç†åçš„æ•°æ®
        """
        logger.info(f"å¼€å§‹æ ‡å‡†åŒ–å¤„ç†ï¼Œæ–¹æ³•: {method}")
        
        df_processed = df.copy()
        
        # åŠ¨æ€è¯†åˆ«å› å­åˆ—ï¼ˆæ’é™¤åŸºç¡€ä¿¡æ¯åˆ—ï¼‰
        factor_columns = [col for col in df.columns if col not in self.exclude_columns]
        
        for col in factor_columns:
            if col not in df_processed.columns:
                continue
                
            series = df_processed[col].dropna()
            if len(series) == 0:
                continue
            
            if method == 'zscore':
                # Z-scoreæ ‡å‡†åŒ–
                mean = series.mean()
                std = series.std()
                if std != 0:
                    df_processed[col] = (df_processed[col] - mean) / std
                else:
                    # å¦‚æœæ ‡å‡†å·®ä¸º0ï¼Œæ‰€æœ‰å€¼è®¾ä¸º0
                    logger.warning(f"å› å­ {col} æ ‡å‡†å·®ä¸º0ï¼Œè®¾ä¸º0")
                    df_processed[col] = 0
            elif method == 'minmax':
                # Min-Maxæ ‡å‡†åŒ–
                min_val = series.min()
                max_val = series.max()
                if max_val != min_val:
                    df_processed[col] = (df_processed[col] - min_val) / (max_val - min_val)
                else:
                    # å¦‚æœæœ€å¤§å€¼ç­‰äºæœ€å°å€¼ï¼Œæ‰€æœ‰å€¼è®¾ä¸º0
                    logger.warning(f"å› å­ {col} æœ€å¤§å€¼ç­‰äºæœ€å°å€¼ï¼Œè®¾ä¸º0")
                    df_processed[col] = 0
            elif method == 'robust':
                # é²æ£’æ ‡å‡†åŒ–ï¼ˆä½¿ç”¨ä¸­ä½æ•°å’ŒMADï¼‰
                median = series.median()
                mad = np.median(np.abs(series - median))
                if mad != 0:
                    df_processed[col] = (df_processed[col] - median) / mad
                else:
                    # å¦‚æœMADä¸º0ï¼Œæ‰€æœ‰å€¼è®¾ä¸º0
                    logger.warning(f"å› å­ {col} MADä¸º0ï¼Œè®¾ä¸º0")
                    df_processed[col] = 0
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ ‡å‡†åŒ–æ–¹æ³•: {method}")
        
        logger.info("æ ‡å‡†åŒ–å¤„ç†å®Œæˆ")
        return df_processed
    
    def select_neutralize_factors(self, df: pd.DataFrame) -> List[str]:
        """
        æ™ºèƒ½é€‰æ‹©éœ€è¦ä¸­æ€§åŒ–çš„å› å­
        
        Args:
            df: å› å­æ•°æ®
            
        Returns:
            éœ€è¦ä¸­æ€§åŒ–çš„å› å­åˆ—è¡¨
        """
        # åŠ¨æ€è¯†åˆ«å› å­åˆ—ï¼ˆæ’é™¤åŸºç¡€ä¿¡æ¯åˆ—ï¼‰
        factor_columns = [col for col in df.columns if col not in self.exclude_columns]
        
        # åŸºäºå› å­åç§°æ¨¡å¼é€‰æ‹©éœ€è¦ä¸­æ€§åŒ–çš„å› å­
        neutralize_patterns = [
            'momentum', 'reversal', 'volatility', 'volume_ratio', 'price_position', 'rsi',
            'pe_ratio', 'pb_ratio', 'ps_ratio', 'pcf_ratio',
            'roe', 'net_profit_margin', 'gross_profit_margin', 'eps',
            'current_ratio', 'quick_ratio', 'cash_ratio', 'debt_to_asset', 'asset_to_equity',
            'cash_to_asset', 'cfo_to_revenue', 'cfo_to_net_profit',
            'receivable_turnover', 'inventory_turnover', 'current_asset_turnover', 'total_asset_turnover',
            'equity_growth', 'asset_growth', 'net_profit_growth', 'eps_growth',
            'dupont_roe', 'dupont_equity_multiplier', 'dupont_asset_turnover',
            'quality_score', 'value_score', 'growth_score'
        ]
        
        neutralize_factors = []
        for factor in factor_columns:
            # æ£€æŸ¥å› å­åæ˜¯å¦åŒ¹é…ä»»ä½•æ¨¡å¼
            if any(pattern in factor.lower() for pattern in neutralize_patterns):
                neutralize_factors.append(factor)
        
        logger.info(f"é€‰æ‹© {len(neutralize_factors)} ä¸ªå› å­è¿›è¡Œä¸­æ€§åŒ–: {neutralize_factors[:10]}{'...' if len(neutralize_factors) > 10 else ''}")
        return neutralize_factors
    
    def neutralize_factors(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä¸­æ€§åŒ–å¤„ç†ï¼ˆå»é™¤è¡Œä¸šå’Œå¸‚å€¼å½±å“ï¼‰
        
        Args:
            df: å› å­æ•°æ®
            
        Returns:
            å¤„ç†åçš„æ•°æ®
        """
        logger.info("å¼€å§‹ä¸­æ€§åŒ–å¤„ç†")
        
        # åŠ¨æ€é€‰æ‹©éœ€è¦ä¸­æ€§åŒ–çš„å› å­
        if not self.neutralize_factor_list:
            self.neutralize_factor_list = self.select_neutralize_factors(df)
        
        df_processed = df.copy()
        
        # è®¡ç®—å¸‚å€¼ï¼ˆä½¿ç”¨æ”¶ç›˜ä»·*æˆäº¤é‡ä½œä¸ºä»£ç†ï¼‰
        df_processed['market_cap_proxy'] = df_processed['close'] * df_processed['volume']
        df_processed['log_market_cap'] = np.log(df_processed['market_cap_proxy'] + 1)
        
        # æŒ‰æ—¥æœŸåˆ†ç»„è¿›è¡Œä¸­æ€§åŒ–
        neutralized_data = []
        
        for date, group in df_processed.groupby('date'):
            group = group.copy()
            
            # å‡†å¤‡ä¸­æ€§åŒ–å˜é‡
            if 'industry' in group.columns:
                # è¡Œä¸šè™šæ‹Ÿå˜é‡
                industry_dummies = pd.get_dummies(group['industry'], prefix='industry')
            else:
                industry_dummies = pd.DataFrame()
            
            # å¸‚å€¼å˜é‡
            market_cap = group['log_market_cap'].fillna(group['log_market_cap'].mean())
            
            # åˆå¹¶ä¸­æ€§åŒ–å˜é‡
            if not industry_dummies.empty:
                X = pd.concat([industry_dummies, market_cap], axis=1)
            else:
                X = market_cap.values.reshape(-1, 1)
            
            # å¯¹æ¯ä¸ªå› å­è¿›è¡Œä¸­æ€§åŒ–
            for factor in self.neutralize_factor_list:
                if factor not in group.columns:
                    continue
                    
                y = group[factor].fillna(group[factor].mean())
                
                if len(y.dropna()) < 10:  # æ•°æ®ç‚¹å¤ªå°‘ï¼Œè·³è¿‡
                    continue
                
                try:
                    # çº¿æ€§å›å½’ä¸­æ€§åŒ–
                    model = LinearRegression()
                    model.fit(X, y)
                    y_pred = model.predict(X)
                    group[f'{factor}_neutralized'] = y - y_pred
                except Exception as e:
                    logger.warning(f"ä¸­æ€§åŒ–å› å­ {factor} å¤±è´¥: {str(e)}")
                    group[f'{factor}_neutralized'] = y
            
            neutralized_data.append(group)
        
        result_df = pd.concat(neutralized_data, ignore_index=True)
        
        # æ›´æ–°å› å­åˆ—å
        neutralized_columns = [f'{col}_neutralized' for col in self.neutralize_factor_list]
        self.factor_columns.extend(neutralized_columns)
        
        logger.info("ä¸­æ€§åŒ–å¤„ç†å®Œæˆ")
        return result_df
    
    def create_factor_ic_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—å› å­ICï¼ˆä¿¡æ¯ç³»æ•°ï¼‰åˆ†æ
        
        Args:
            df: å› å­æ•°æ®
            
        Returns:
            åŒ…å«ICåˆ†æçš„æ•°æ®
        """
        logger.info("å¼€å§‹è®¡ç®—å› å­ICåˆ†æ")
        
        df_processed = df.copy()
        
        # åŠ¨æ€è¯†åˆ«å› å­åˆ—ï¼ˆæ’é™¤åŸºç¡€ä¿¡æ¯åˆ—ï¼‰
        factor_columns = [col for col in df.columns if col not in self.exclude_columns]
        
        # è®¡ç®—æœªæ¥æ”¶ç›Šç‡ï¼ˆä½¿ç”¨ä¸‹ä¸€å¤©çš„æ”¶ç›Šç‡ï¼‰
        df_processed = df_processed.sort_values(['code', 'date'])
        df_processed['future_return'] = df_processed.groupby('code')['pctChg'].shift(-1)
        
        # æŒ‰æ—¥æœŸåˆ†ç»„è®¡ç®—IC
        ic_results = []
        
        for date, group in df_processed.groupby('date'):
            if group['future_return'].isnull().all():
                continue
                
            date_ic = {'date': date}
            
            for factor in factor_columns:
                if factor not in group.columns:
                    continue
                    
                # è®¡ç®—ICï¼ˆSpearmanç›¸å…³ç³»æ•°ï¼‰
                factor_values = group[factor].dropna()
                future_returns = group.loc[factor_values.index, 'future_return'].dropna()
                
                if len(factor_values) > 5 and len(future_returns) > 5:
                    # ç¡®ä¿ç´¢å¼•å¯¹é½
                    common_idx = factor_values.index.intersection(future_returns.index)
                    if len(common_idx) > 5:
                        ic = factor_values.loc[common_idx].corr(future_returns.loc[common_idx], method='spearman')
                        date_ic[f'{factor}_ic'] = ic
            
            ic_results.append(date_ic)
        
        ic_df = pd.DataFrame(ic_results)
        logger.info(f"ICåˆ†æå®Œæˆï¼Œè®¡ç®—äº† {len(ic_df)} ä¸ªäº¤æ˜“æ—¥")
        
        return ic_df
    
    def save_preprocessed_factors(self, df: pd.DataFrame, table_name: str = 'dws_stock_factors_preprocessed'):
        """
        åŠ¨æ€ä¿å­˜é¢„å¤„ç†åçš„å› å­æ•°æ®
        
        Args:
            df: é¢„å¤„ç†åçš„å› å­æ•°æ®
            table_name: ç›®æ ‡è¡¨å
        """
        logger.info(f"ä¿å­˜é¢„å¤„ç†åçš„å› å­æ•°æ®åˆ°è¡¨: {table_name}")
        logger.info(f"æ•°æ®åŒ…å« {len(df.columns)} åˆ—: {list(df.columns)}")
        
        try:
            # åŠ¨æ€åˆ›å»ºé¢„å¤„ç†å› å­è¡¨
            self.create_preprocessed_table(df, table_name)
            
            # åˆ†æ‰¹ä¿å­˜æ•°æ®
            batch_size = 1000
            total_records = len(df)
            
            for i in range(0, total_records, batch_size):
                batch_df = df.iloc[i:i+batch_size]
                batch_data = batch_df.to_dict('records')
                
                self.db_manager.upsert_data_safe(table_name, batch_data, ['code', 'date'])
                logger.info(f"å·²ä¿å­˜ {min(i+batch_size, total_records)}/{total_records} æ¡è®°å½•")
            
            logger.info(f"é¢„å¤„ç†å› å­æ•°æ®ä¿å­˜æˆåŠŸ: {total_records} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"ä¿å­˜é¢„å¤„ç†å› å­æ•°æ®å¤±è´¥: {str(e)}")
            raise
    
    def drop_table_if_exists(self, table_name: str):
        """åˆ é™¤è¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
        try:
            with self.db_manager.engine.connect() as conn:
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                check_query = f"SHOW TABLES LIKE '{table_name}'"
                result = conn.execute(text(check_query))
                if result.fetchone():
                    # è¡¨å­˜åœ¨ï¼Œåˆ é™¤å®ƒ
                    drop_query = f"DROP TABLE {table_name}"
                    conn.execute(text(drop_query))
                    conn.commit()
                    logger.info(f"å·²åˆ é™¤ç°æœ‰è¡¨: {table_name}")
                else:
                    logger.info(f"è¡¨ {table_name} ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤")
        except Exception as e:
            logger.warning(f"åˆ é™¤è¡¨ {table_name} æ—¶å‡ºé”™: {str(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œ

    def create_preprocessed_table(self, df: pd.DataFrame, table_name: str):
        """åŠ¨æ€åˆ›å»ºé¢„å¤„ç†å› å­è¡¨"""
        try:
            # å…ˆåˆ é™¤å·²å­˜åœ¨çš„è¡¨
            self.drop_table_if_exists(table_name)
            
            # è·å–DataFrameçš„åˆ—ä¿¡æ¯
            columns_info = []
            
            # é¦–å…ˆæ·»åŠ ä¸»é”®åˆ—
            columns_info.append("code VARCHAR(20) NOT NULL COMMENT 'è‚¡ç¥¨ä»£ç '")
            columns_info.append("date DATE NOT NULL COMMENT 'æ—¥æœŸ'")
            
            # ç„¶åæ·»åŠ å…¶ä»–åˆ—ï¼Œæ ¹æ®å®é™…æ•°æ®ç±»å‹åŠ¨æ€åˆ›å»º
            for col in df.columns:
                if col in ['code', 'date']:
                    continue
                elif df[col].dtype in ['int64', 'float64']:
                    columns_info.append(f"{col} DECIMAL(20,6) COMMENT '{col}'")
                elif df[col].dtype == 'object':
                    columns_info.append(f"{col} VARCHAR(200) COMMENT '{col}'")
                else:
                    columns_info.append(f"{col} TEXT COMMENT '{col}'")
            
            # æ·»åŠ ä¸»é”®çº¦æŸ
            columns_info.append("PRIMARY KEY (code, date)")
            
            logger.info(f"åˆ›å»ºé¢„å¤„ç†è¡¨ {table_name}ï¼ŒåŒ…å« {len(columns_info)} åˆ—")
            self.db_manager.create_table_safe(table_name, columns_info)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºé¢„å¤„ç†å› å­è¡¨å¤±è´¥: {str(e)}")
            raise
    
    def run_full_preprocessing(self, start_date: str, end_date: str, 
                             table_name: str = 'dws_stock_factors',
                             output_table_name: str = None,
                             missing_method: str = 'forward_fill',
                             winsorize_method: str = 'quantile',
                             winsorize_limits: Tuple[float, float] = (0.01, 0.99),
                             standardize_method: str = 'zscore',
                             neutralize: bool = True,
                             ic_analysis: bool = True) -> pd.DataFrame:
        """
        è¿è¡Œå®Œæ•´çš„å› å­é¢„å¤„ç†æµç¨‹
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            table_name: è¾“å…¥å› å­è¡¨å
            output_table_name: è¾“å‡ºè¡¨åï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
            missing_method: ç¼ºå¤±å€¼å¤„ç†æ–¹æ³•
            winsorize_method: å»æå€¼æ–¹æ³•
            winsorize_limits: å»æå€¼é™åˆ¶
            standardize_method: æ ‡å‡†åŒ–æ–¹æ³•
            neutralize: æ˜¯å¦è¿›è¡Œä¸­æ€§åŒ–
            ic_analysis: æ˜¯å¦è¿›è¡ŒICåˆ†æ
            
        Returns:
            é¢„å¤„ç†åçš„å› å­æ•°æ®
        """
        logger.info("=== å¼€å§‹å› å­é¢„å¤„ç†æµç¨‹ ===")
        
        # åŠ¨æ€ç”Ÿæˆè¾“å‡ºè¡¨å
        if output_table_name is None:
            output_table_name = f"{table_name}_preprocessed"
        
        logger.info(f"è¾“å…¥è¡¨: {table_name}")
        logger.info(f"è¾“å‡ºè¡¨: {output_table_name}")
        
        try:
            # 1. è·å–å› å­æ•°æ®
            logger.info("æ­¥éª¤1: è·å–å› å­æ•°æ®")
            df = self.get_factor_data(start_date, end_date, table_name)
            
            # 2. å¤„ç†ç¼ºå¤±å€¼
            logger.info("æ­¥éª¤2: å¤„ç†ç¼ºå¤±å€¼")
            df = self.handle_missing_values(df, method=missing_method)
            
            # 3. å»æå€¼å¤„ç†
            logger.info("æ­¥éª¤3: å»æå€¼å¤„ç†")
            df = self.winsorize_factors(df, method=winsorize_method, limits=winsorize_limits)
            
            # 4. æ ‡å‡†åŒ–å¤„ç†
            logger.info("æ­¥éª¤4: æ ‡å‡†åŒ–å¤„ç†")
            df = self.standardize_factors(df, method=standardize_method)
            
            # 5. ä¸­æ€§åŒ–å¤„ç†
            if neutralize:
                logger.info("æ­¥éª¤5: ä¸­æ€§åŒ–å¤„ç†")
                df = self.neutralize_factors(df)
            
            # 6. ICåˆ†æ
            if ic_analysis:
                logger.info("æ­¥éª¤6: ICåˆ†æ")
                ic_df = self.create_factor_ic_analysis(df)
                # ä¿å­˜ICåˆ†æç»“æœ
                self.save_ic_analysis(ic_df)
            
            # 7. ä¿å­˜é¢„å¤„ç†ç»“æœ
            logger.info("æ­¥éª¤7: ä¿å­˜é¢„å¤„ç†ç»“æœ")
            self.save_preprocessed_factors(df, output_table_name)
            
            logger.info("=== å› å­é¢„å¤„ç†æµç¨‹å®Œæˆ ===")
            logger.info(f"é¢„å¤„ç†ç»“æœå·²ä¿å­˜åˆ°è¡¨: {output_table_name}")
            return df
            
        except Exception as e:
            logger.error(f"å› å­é¢„å¤„ç†å¤±è´¥: {str(e)}")
            raise
    
    def save_ic_analysis(self, ic_df: pd.DataFrame, table_name: str = 'factor_ic_analysis'):
        """ä¿å­˜ICåˆ†æç»“æœ"""
        try:
            # å…ˆåˆ é™¤å·²å­˜åœ¨çš„ICåˆ†æè¡¨
            self.drop_table_if_exists(table_name)
            
            # åˆ›å»ºICåˆ†æè¡¨
            columns_info = [
                "date DATE NOT NULL COMMENT 'æ—¥æœŸ'",
                "PRIMARY KEY (date)"
            ]
            
            # æ·»åŠ ICåˆ—
            for col in ic_df.columns:
                if col != 'date':
                    columns_info.append(f"{col} DECIMAL(10,6) COMMENT '{col}'")
            
            self.db_manager.create_table_safe(table_name, columns_info)
            
            # ä¿å­˜æ•°æ®
            batch_data = ic_df.to_dict('records')
            self.db_manager.upsert_data_safe(table_name, batch_data, ['date'])
            
            logger.info(f"ICåˆ†æç»“æœä¿å­˜æˆåŠŸ: {len(ic_df)} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ICåˆ†æç»“æœå¤±è´¥: {str(e)}")
            raise
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        self.db_manager.close()


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å› å­é¢„å¤„ç†å™¨')
    parser.add_argument('--start-date', default='2020-06-01', help='å¼€å§‹æ—¥æœŸ')
    parser.add_argument('--end-date', help='ç»“æŸæ—¥æœŸ')
    parser.add_argument('--table-name', default='dws_stock_factors', help='è¾“å…¥å› å­è¡¨å')
    parser.add_argument('--output-table-name', help='è¾“å‡ºè¡¨åï¼ˆé»˜è®¤ï¼š{table_name}_preprocessedï¼‰')
    parser.add_argument('--missing-method', choices=['forward_fill', 'backward_fill', 'mean', 'median', 'drop'], 
                       default='forward_fill', help='ç¼ºå¤±å€¼å¤„ç†æ–¹æ³•')
    parser.add_argument('--winsorize-method', choices=['quantile', 'std', 'mad'], 
                       default='quantile', help='å»æå€¼æ–¹æ³•')
    parser.add_argument('--winsorize-limits', nargs=2, type=float, default=[0.01, 0.99], 
                       help='å»æå€¼é™åˆ¶')
    parser.add_argument('--standardize-method', choices=['zscore', 'minmax', 'robust'], 
                       default='zscore', help='æ ‡å‡†åŒ–æ–¹æ³•')
    parser.add_argument('--no-neutralize', action='store_true', help='è·³è¿‡ä¸­æ€§åŒ–å¤„ç†')
    parser.add_argument('--no-ic-analysis', action='store_true', help='è·³è¿‡ICåˆ†æ')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    preprocessor = FactorPreprocessor()
    
    try:
        # è¿è¡Œå®Œæ•´é¢„å¤„ç†æµç¨‹
        df = preprocessor.run_full_preprocessing(
            start_date=args.start_date,
            end_date=args.end_date,
            table_name=args.table_name,
            output_table_name=args.output_table_name,
            missing_method=args.missing_method,
            winsorize_method=args.winsorize_method,
            winsorize_limits=tuple(args.winsorize_limits),
            standardize_method=args.standardize_method,
            neutralize=not args.no_neutralize,
            ic_analysis=not args.no_ic_analysis
        )
        
        print(f"\nğŸ“Š å› å­é¢„å¤„ç†å®Œæˆ:")
        print(f"  æ€»è®°å½•æ•°: {len(df):,}")
        print(f"  å› å­æ•°é‡: {len([col for col in df.columns if col not in ['code', 'date', 'industry', 'code_name']]):,}")
        print(f"  ä¿å­˜åˆ°è¡¨: dws_stock_factors_preprocessed")
        
    except Exception as e:
        print(f"âŒ å› å­é¢„å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        preprocessor.close()


if __name__ == '__main__':
    main()
